{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Import important libraries  \n",
    "import requests               # To send HTTP requests to the GitHub API \n",
    "import pandas as pd           # To handle tabular data and CSV operations\n",
    "import time                   # To add delays and manage rate limits\n",
    "import logging                # To record what the script is doing at every given point (log errors, warnings, and process steps)\n",
    "from tqdm import tqdm         # To show a progress bar while the script runs\n",
    "from pathlib import Path      # To manage file paths\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: Setup access to GitHub \n",
    "\n",
    "Token = os.getenv(\"GITHUB_PERSONAL_ACCESS_TOKEN\")  #If you intend to run this code, this is where you'll add your token\n",
    "ORG = \"opensafely\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {Token}\",\n",
    "    \"Accept\": \"application/vnd.github+json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Step 3: Define search parameters to extract all repos and ehrQL from GitHub under OpenSafely Org.and set storage.\n",
    "\n",
    "query = \"ehrQL+language:python+org:opensafely\"  # search for code files containing ehrQL in Python within OpenSafely org\n",
    "base_url = \"https://api.github.com/search/code\"\n",
    "\n",
    "per_page = 100 \n",
    "max_pages = 5\n",
    "\n",
    "exclude_keywords = ['documentation', 'research-template', 'tutorials']\n",
    "\n",
    "repo_creation_cache = {}  # Cache to avoid fetching repo info multiple times\n",
    "all_results = [] #list for saving results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching GitHub - Page 1\n"
     ]
    }
   ],
   "source": [
    "#Step 4: Loop through GitHub search pages\n",
    "        #Process each search result and skip repos with the excluded keywords defined earlier. \n",
    "        #Extract the creation date for repos (will be useful for sorting repos in the streamlit app)\n",
    "\n",
    "for page in range(1, max_pages + 1):\n",
    "    print(f\"Searching GitHub - Page {page}\")\n",
    "    url = f\"{base_url}?q={query}&per_page={per_page}&page={page}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error fetching page {page}: {response.status_code}, message: {response.text}\")\n",
    "        break\n",
    "    \n",
    "    results = response.json().get(\"items\", [])\n",
    "    if not results:\n",
    "        print(\"No more results.\")\n",
    "        break\n",
    "    \n",
    "    for item in results:\n",
    "        repo_name = item[\"repository\"][\"full_name\"].lower()\n",
    "        \n",
    "       \n",
    "        if any(keyword in repo_name for keyword in exclude_keywords):   # Skip repos with excluded keywords\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        if repo_name in repo_creation_cache:    # Get repo creation date (cached for efficiency)\n",
    "            created_on = repo_creation_cache[repo_name]\n",
    "        else:\n",
    "            repo_api_url = f\"https://api.github.com/repos/{repo_name}\"\n",
    "            repo_resp = requests.get(repo_api_url, headers=HEADERS)\n",
    "            created_on = repo_resp.json().get(\"created_at\") if repo_resp.status_code == 200 else None\n",
    "            repo_creation_cache[repo_name] = created_on\n",
    "        \n",
    "    \n",
    "        file_url = item[\"html_url\"]\n",
    "        raw_url = file_url.replace(\"github.com\", \"raw.githubusercontent.com\").replace(\"/blob/\", \"/\")\n",
    "    \n",
    "        all_results.append({\n",
    "            \"Repository\": item[\"repository\"][\"full_name\"],  \n",
    "            \"Created_on\": created_on,                       \n",
    "            \"File_Name\": item[\"name\"],\n",
    "            \"File_Path\": item[\"path\"],\n",
    "            \"File_URL\": file_url,\n",
    "            \"Raw_URL\": raw_url\n",
    "        })\n",
    "        \n",
    "        time.sleep(1)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 301 records to opensafely_ehrql_code_files.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 5:Create fiile URLs, append results in the desired column order and save\n",
    "\n",
    "df = pd.DataFrame(all_results)\n",
    "\n",
    "output_path = \"opensafely_ehrql_code_files.csv\"\n",
    "\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Saved {len(all_results)} records to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Load features exactly as written\n",
    "feature_file = Path(\"ehrQL_features.txt\")\n",
    "features_to_search = [line.strip() for line in feature_file.read_text(encoding=\"utf-8\").splitlines() if line.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Set up logging, Prepare download directory and DataFrame\n",
    "logging.basicConfig(\n",
    "    filename='feature_search.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "download_dir = Path(\"downloaded_files\")\n",
    "download_dir.mkdir(exist_ok=True)\n",
    "\n",
    "df = df.drop_duplicates(subset=[\"Raw_URL\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Initialise counters and repo map\n",
    "feature_counts = {feature: 0 for feature in features_to_search}\n",
    "feature_repo_map = {feature: set() for feature in features_to_search}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading files: 100%|██████████| 301/301 [4:23:31<00:00, 52.53s/it]      \n"
     ]
    }
   ],
   "source": [
    "# Step 7: Download raw files\n",
    "logging.info(\"Starting file downloads...\")\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Downloading files\"):\n",
    "    raw_url = row[\"Raw_URL\"]\n",
    "    file_path = download_dir / f\"file_{idx}.txt\"\n",
    "\n",
    "    if not file_path.exists():\n",
    "        try:\n",
    "            resp = requests.get(raw_url, timeout=20)\n",
    "            if resp.status_code == 200:\n",
    "                file_path.write_text(resp.text, encoding=\"utf-8\")\n",
    "            else:\n",
    "                logging.warning(f\"Failed to fetch file (status {resp.status_code}) | URL: {raw_url}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error fetching {raw_url} | Reason: {e}\")\n",
    "        time.sleep(0.4)\n",
    "\n",
    "logging.info(\"All files downloaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing files: 100%|██████████| 301/301 [00:11<00:00, 26.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Parse files using slicing (full scan)\n",
    "\n",
    "logging.info(\"Starting feature parsing...\")\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Parsing files\"):\n",
    "    repo_name = row[\"Repository\"]\n",
    "    file_path = download_dir / f\"file_{idx}.txt\"\n",
    "\n",
    "    try:\n",
    "        try:\n",
    "            file_content = file_path.read_text(encoding=\"utf-8\")\n",
    "        except UnicodeDecodeError:\n",
    "            file_content = file_path.read_text(encoding=\"latin-1\", errors=\"ignore\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading {file_path} | Reason: {e}\")\n",
    "        continue\n",
    "\n",
    "    content_lower = file_content.casefold()\n",
    "    text_len = len(content_lower)\n",
    "\n",
    "    for feature in features_to_search:\n",
    "        feature_lower = feature.casefold()\n",
    "        base = feature_lower.split(\"(\")[0]  # part before arguments\n",
    "        base_len = len(base)\n",
    "        start = 0\n",
    "        count = 0\n",
    "\n",
    "        while True:\n",
    "            pos = content_lower.find(base, start)\n",
    "            if pos == -1:\n",
    "                break\n",
    "\n",
    "            end_pos = pos + base_len\n",
    "            temp_pos = end_pos\n",
    "# (This part of the code wasn't implemented in the script)\n",
    "#If you want to extend this code, this is where you should think of parenthesis and arguments.\n",
    "            # Skip optional whitespace before '('(possibly ignore)\n",
    "            # while temp_pos < text_len and content_lower[temp_pos].isspace():\n",
    "            #     temp_pos += 1\n",
    "\n",
    "            # # If '(' follows, treat as full function call\n",
    "            # if temp_pos < text_len and content_lower[temp_pos] == \"(\":\n",
    "            #     close_pos = content_lower.find(\")\", temp_pos)\n",
    "            #     if close_pos != -1:\n",
    "            #         count += 1\n",
    "            #         start = close_pos + 1   #at the moment, this works for the scope of this project\n",
    "            # else:                           # Count even if no arguments\n",
    "            count += 1\n",
    "            start = end_pos\n",
    "\n",
    "        if count > 0:\n",
    "            feature_counts[feature] += count\n",
    "            feature_repo_map[feature].add(repo_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files saved: ehrQL_feature_counts.csv, feature-repo_map.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Save counts\n",
    "df_counts = pd.DataFrame(\n",
    "    [{\"Feature\": feat, \"Count\": feature_counts[feat]} for feat in features_to_search],\n",
    "    columns=[\"Feature\", \"Count\"]\n",
    ")\n",
    "df_counts.to_csv(\"ehrQL_feature_counts.csv\", index=False)\n",
    "\n",
    "repo_rows = []\n",
    "for feat, repos in feature_repo_map.items():\n",
    "    for repo in sorted(repos):\n",
    "        repo_rows.append({\"Feature\": feat, \"Repository\": repo, \"Raw_URL\": raw_url})\n",
    "\n",
    "df_repos = pd.DataFrame(repo_rows, columns=[\"Feature\", \"Repository, Raw_URL\"])\n",
    "df_repos.to_csv(\"feature-repo_map.csv\", index=False)\n",
    "\n",
    "logging.info(\"Feature counts and repo map exported.\")\n",
    "print(\"Files saved\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
