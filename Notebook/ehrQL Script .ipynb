{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Import important libraries  \n",
    "import requests               # To send HTTP requests to the GitHub API \n",
    "import pandas as pd           # To handle tabular data and CSV operations\n",
    "import time                   # To add delays and manage rate limits\n",
    "import logging                # To record what the script is doing at every given point (log errors, warnings, and process steps)\n",
    "from tqdm import tqdm         # To show a progress bar while the script runs\n",
    "from pathlib import Path      # To manage file paths\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: Setup access to GitHub \n",
    "\n",
    "Token = os.getenv(\"GITHUB_PERSONAL_ACCESS_TOKEN\")  #This is where you'll add your token\n",
    "ORG = \"opensafely\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {Token}\",\n",
    "    \"Accept\": \"application/vnd.github+json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Step 3: Define search parameters to extract all repos and ehrQL from GitHub under OpenSafely Org.and set storage.\n",
    "\n",
    "query = \"ehrQL+language:python+org:opensafely\"  # search for code files containing ehrQL in Python within OpenSafely org\n",
    "base_url = \"https://api.github.com/search/code\"\n",
    "\n",
    "per_page = 100  # Pagination: max 100 results per page allowed by GitHub API\n",
    "max_pages = 5\n",
    "\n",
    "exclude_keywords = ['documentation', 'research-template', 'tutorials']\n",
    "\n",
    "repo_creation_cache = {}  # Cache to avoid fetching repo info multiple times\n",
    "all_results = [] #list for saving results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching GitHub - Page 1\n",
      "Searching GitHub - Page 2\n",
      "Searching GitHub - Page 3\n",
      "Searching GitHub - Page 4\n",
      "Searching GitHub - Page 5\n",
      "No more results.\n"
     ]
    }
   ],
   "source": [
    "#Step 4: Loop through GitHub search pages\n",
    "        #Process each search result and skip repos with the excluded keywords defined earlier. \n",
    "        #Extract the creation date for repos (will be useful for sorting repos in the streamlit app)\n",
    "\n",
    "for page in range(1, max_pages + 1):\n",
    "    print(f\"Searching GitHub - Page {page}\")\n",
    "    url = f\"{base_url}?q={query}&per_page={per_page}&page={page}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error fetching page {page}: {response.status_code}, message: {response.text}\")\n",
    "        break\n",
    "    \n",
    "    results = response.json().get(\"items\", [])\n",
    "    if not results:\n",
    "        print(\"No more results.\")\n",
    "        break\n",
    "    \n",
    "    for item in results:\n",
    "        repo_name = item[\"repository\"][\"full_name\"].lower()\n",
    "        \n",
    "       \n",
    "        if any(keyword in repo_name for keyword in exclude_keywords):   # Skip repos with excluded keywords\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        if repo_name in repo_creation_cache:    # Get repo creation date (cached for efficiency)\n",
    "            created_on = repo_creation_cache[repo_name]\n",
    "        else:\n",
    "            repo_api_url = f\"https://api.github.com/repos/{repo_name}\"\n",
    "            repo_resp = requests.get(repo_api_url, headers=HEADERS)\n",
    "            created_on = repo_resp.json().get(\"created_at\") if repo_resp.status_code == 200 else None\n",
    "            repo_creation_cache[repo_name] = created_on\n",
    "        \n",
    "        # File URLs\n",
    "        file_url = item[\"html_url\"]\n",
    "        raw_url = file_url.replace(\"github.com\", \"raw.githubusercontent.com\").replace(\"/blob/\", \"/\")\n",
    "        \n",
    "        # Append results in the desired column order\n",
    "        all_results.append({\n",
    "            \"Repository\": item[\"repository\"][\"full_name\"],  \n",
    "            \"Created_on\": created_on,                       \n",
    "            \"File_Name\": item[\"name\"],\n",
    "            \"File_Path\": item[\"path\"],\n",
    "            \"File_URL\": file_url,\n",
    "            \"Raw_URL\": raw_url\n",
    "        })\n",
    "        \n",
    "        time.sleep(1)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 301 records to opensafely_ehrql_code_files.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 5:Create fiile URLs, append results in the desired column order and save\n",
    "\n",
    "df = pd.DataFrame(all_results)\n",
    "\n",
    "output_path = \"opensafely_ehrql_code_files.csv\"\n",
    "\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Saved {len(all_results)} records to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Repository</th>\n",
       "      <th>Created_on</th>\n",
       "      <th>File_Name</th>\n",
       "      <th>File_Path</th>\n",
       "      <th>File_URL</th>\n",
       "      <th>Raw_URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>opensafely/cis-pop-validation-ehrql</td>\n",
       "      <td>2022-09-29T15:19:10Z</td>\n",
       "      <td>codelists_ehrql.py</td>\n",
       "      <td>analysis/codelists_ehrql.py</td>\n",
       "      <td>https://github.com/opensafely/cis-pop-validati...</td>\n",
       "      <td>https://raw.githubusercontent.com/opensafely/c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>opensafely/dummy-data-workshop</td>\n",
       "      <td>2024-11-19T08:39:46Z</td>\n",
       "      <td>ehrql_dataset_definition.py</td>\n",
       "      <td>analysis/ehrql_dataset_definition.py</td>\n",
       "      <td>https://github.com/opensafely/dummy-data-works...</td>\n",
       "      <td>https://raw.githubusercontent.com/opensafely/d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>opensafely/early-inflammatory-arthritis</td>\n",
       "      <td>2022-02-22T10:53:22Z</td>\n",
       "      <td>codelists_ehrQL.py</td>\n",
       "      <td>analysis/codelists_ehrQL.py</td>\n",
       "      <td>https://github.com/opensafely/early-inflammato...</td>\n",
       "      <td>https://raw.githubusercontent.com/opensafely/e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>opensafely/asthma_sro</td>\n",
       "      <td>2022-02-07T12:00:24Z</td>\n",
       "      <td>ehrql_codelists_ast.py</td>\n",
       "      <td>analysis/ehrQL_code/ehrql_codelists_ast.py</td>\n",
       "      <td>https://github.com/opensafely/asthma_sro/blob/...</td>\n",
       "      <td>https://raw.githubusercontent.com/opensafely/a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>opensafely/end-of-life-carequality</td>\n",
       "      <td>2023-08-15T14:45:00Z</td>\n",
       "      <td>dataset_definition_ehrql_example.py</td>\n",
       "      <td>analysis/ehrql/dataset_definition_ehrql_exampl...</td>\n",
       "      <td>https://github.com/opensafely/end-of-life-care...</td>\n",
       "      <td>https://raw.githubusercontent.com/opensafely/e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>opensafely/covid-vaccine-history</td>\n",
       "      <td>2024-10-08T15:30:34Z</td>\n",
       "      <td>codelists.py</td>\n",
       "      <td>analysis/1-extract/codelists.py</td>\n",
       "      <td>https://github.com/opensafely/covid-vaccine-hi...</td>\n",
       "      <td>https://raw.githubusercontent.com/opensafely/c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>opensafely/asthma_sro</td>\n",
       "      <td>2022-02-07T12:00:24Z</td>\n",
       "      <td>ehrql_measures_test2.py</td>\n",
       "      <td>analysis/ehrQL_code/ehrql_measures_test2.py</td>\n",
       "      <td>https://github.com/opensafely/asthma_sro/blob/...</td>\n",
       "      <td>https://raw.githubusercontent.com/opensafely/a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>opensafely/pifu-data-exploration</td>\n",
       "      <td>2025-05-16T08:42:05Z</td>\n",
       "      <td>measures.py</td>\n",
       "      <td>analysis/measures.py</td>\n",
       "      <td>https://github.com/opensafely/pifu-data-explor...</td>\n",
       "      <td>https://raw.githubusercontent.com/opensafely/p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>opensafely/comparative-booster-ehrql-poc</td>\n",
       "      <td>2023-01-11T12:21:15Z</td>\n",
       "      <td>test_variables_lib.py</td>\n",
       "      <td>analysis/test_variables_lib.py</td>\n",
       "      <td>https://github.com/opensafely/comparative-boos...</td>\n",
       "      <td>https://raw.githubusercontent.com/opensafely/c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>opensafely/post-covid-respiratory</td>\n",
       "      <td>2022-07-05T15:54:02Z</td>\n",
       "      <td>dataset_definition_vax.py</td>\n",
       "      <td>analysis/dataset_definition/dataset_definition...</td>\n",
       "      <td>https://github.com/opensafely/post-covid-respi...</td>\n",
       "      <td>https://raw.githubusercontent.com/opensafely/p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>301 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Repository            Created_on  \\\n",
       "0         opensafely/cis-pop-validation-ehrql  2022-09-29T15:19:10Z   \n",
       "1              opensafely/dummy-data-workshop  2024-11-19T08:39:46Z   \n",
       "2     opensafely/early-inflammatory-arthritis  2022-02-22T10:53:22Z   \n",
       "3                       opensafely/asthma_sro  2022-02-07T12:00:24Z   \n",
       "4          opensafely/end-of-life-carequality  2023-08-15T14:45:00Z   \n",
       "..                                        ...                   ...   \n",
       "296          opensafely/covid-vaccine-history  2024-10-08T15:30:34Z   \n",
       "297                     opensafely/asthma_sro  2022-02-07T12:00:24Z   \n",
       "298          opensafely/pifu-data-exploration  2025-05-16T08:42:05Z   \n",
       "299  opensafely/comparative-booster-ehrql-poc  2023-01-11T12:21:15Z   \n",
       "300         opensafely/post-covid-respiratory  2022-07-05T15:54:02Z   \n",
       "\n",
       "                               File_Name  \\\n",
       "0                     codelists_ehrql.py   \n",
       "1            ehrql_dataset_definition.py   \n",
       "2                     codelists_ehrQL.py   \n",
       "3                 ehrql_codelists_ast.py   \n",
       "4    dataset_definition_ehrql_example.py   \n",
       "..                                   ...   \n",
       "296                         codelists.py   \n",
       "297              ehrql_measures_test2.py   \n",
       "298                          measures.py   \n",
       "299                test_variables_lib.py   \n",
       "300            dataset_definition_vax.py   \n",
       "\n",
       "                                             File_Path  \\\n",
       "0                          analysis/codelists_ehrql.py   \n",
       "1                 analysis/ehrql_dataset_definition.py   \n",
       "2                          analysis/codelists_ehrQL.py   \n",
       "3           analysis/ehrQL_code/ehrql_codelists_ast.py   \n",
       "4    analysis/ehrql/dataset_definition_ehrql_exampl...   \n",
       "..                                                 ...   \n",
       "296                    analysis/1-extract/codelists.py   \n",
       "297        analysis/ehrQL_code/ehrql_measures_test2.py   \n",
       "298                               analysis/measures.py   \n",
       "299                     analysis/test_variables_lib.py   \n",
       "300  analysis/dataset_definition/dataset_definition...   \n",
       "\n",
       "                                              File_URL  \\\n",
       "0    https://github.com/opensafely/cis-pop-validati...   \n",
       "1    https://github.com/opensafely/dummy-data-works...   \n",
       "2    https://github.com/opensafely/early-inflammato...   \n",
       "3    https://github.com/opensafely/asthma_sro/blob/...   \n",
       "4    https://github.com/opensafely/end-of-life-care...   \n",
       "..                                                 ...   \n",
       "296  https://github.com/opensafely/covid-vaccine-hi...   \n",
       "297  https://github.com/opensafely/asthma_sro/blob/...   \n",
       "298  https://github.com/opensafely/pifu-data-explor...   \n",
       "299  https://github.com/opensafely/comparative-boos...   \n",
       "300  https://github.com/opensafely/post-covid-respi...   \n",
       "\n",
       "                                               Raw_URL  \n",
       "0    https://raw.githubusercontent.com/opensafely/c...  \n",
       "1    https://raw.githubusercontent.com/opensafely/d...  \n",
       "2    https://raw.githubusercontent.com/opensafely/e...  \n",
       "3    https://raw.githubusercontent.com/opensafely/a...  \n",
       "4    https://raw.githubusercontent.com/opensafely/e...  \n",
       "..                                                 ...  \n",
       "296  https://raw.githubusercontent.com/opensafely/c...  \n",
       "297  https://raw.githubusercontent.com/opensafely/a...  \n",
       "298  https://raw.githubusercontent.com/opensafely/p...  \n",
       "299  https://raw.githubusercontent.com/opensafely/c...  \n",
       "300  https://raw.githubusercontent.com/opensafely/p...  \n",
       "\n",
       "[301 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Load features exactly as written\n",
    "feature_file = Path(\"ehrQL_features.txt\")\n",
    "features_to_search = [line.strip() for line in feature_file.read_text(encoding=\"utf-8\").splitlines() if line.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Set up logging, Prepare download directory and DataFrame\n",
    "logging.basicConfig(\n",
    "    filename='feature_search.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "download_dir = Path(\"downloaded_files\")\n",
    "download_dir.mkdir(exist_ok=True)\n",
    "\n",
    "df = df.drop_duplicates(subset=[\"Raw_URL\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Initialise counters and repo map\n",
    "feature_counts = {feature: 0 for feature in features_to_search}\n",
    "feature_repo_map = {feature: set() for feature in features_to_search}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading files: 100%|██████████| 301/301 [4:23:31<00:00, 52.53s/it]      \n"
     ]
    }
   ],
   "source": [
    "# Step 7: Download raw files\n",
    "logging.info(\"Starting file downloads...\")\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Downloading files\"):\n",
    "    raw_url = row[\"Raw_URL\"]\n",
    "    file_path = download_dir / f\"file_{idx}.txt\"\n",
    "\n",
    "    if not file_path.exists():\n",
    "        try:\n",
    "            resp = requests.get(raw_url, timeout=20)\n",
    "            if resp.status_code == 200:\n",
    "                file_path.write_text(resp.text, encoding=\"utf-8\")\n",
    "            else:\n",
    "                logging.warning(f\"Failed to fetch file (status {resp.status_code}) | URL: {raw_url}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error fetching {raw_url} | Reason: {e}\")\n",
    "        time.sleep(0.4)\n",
    "\n",
    "logging.info(\"All files downloaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing files: 100%|██████████| 301/301 [00:11<00:00, 26.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Parse files using slicing (full scan)\n",
    "\n",
    "logging.info(\"Starting feature parsing...\")\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Parsing files\"):\n",
    "    repo_name = row[\"Repository\"]\n",
    "    file_path = download_dir / f\"file_{idx}.txt\"\n",
    "\n",
    "    try:\n",
    "        try:\n",
    "            file_content = file_path.read_text(encoding=\"utf-8\")\n",
    "        except UnicodeDecodeError:\n",
    "            file_content = file_path.read_text(encoding=\"latin-1\", errors=\"ignore\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading {file_path} | Reason: {e}\")\n",
    "        continue\n",
    "\n",
    "    content_lower = file_content.casefold()\n",
    "    text_len = len(content_lower)\n",
    "\n",
    "    for feature in features_to_search:\n",
    "        feature_lower = feature.casefold()\n",
    "        base = feature_lower.split(\"(\")[0]  # part before arguments\n",
    "        base_len = len(base)\n",
    "        start = 0\n",
    "        count = 0\n",
    "\n",
    "        while True:\n",
    "            pos = content_lower.find(base, start)\n",
    "            if pos == -1:\n",
    "                break\n",
    "\n",
    "            end_pos = pos + base_len\n",
    "            temp_pos = end_pos\n",
    "#Write out a summary of what the code below was intended for (it wasn't implemented in the script)\n",
    "#If you want to extend this code, this is where you should think of parenthesis and arguments.\n",
    "            # Skip optional whitespace before '('(possibly ignore)\n",
    "            # while temp_pos < text_len and content_lower[temp_pos].isspace():\n",
    "            #     temp_pos += 1\n",
    "\n",
    "            # # If '(' follows, treat as full function call\n",
    "            # if temp_pos < text_len and content_lower[temp_pos] == \"(\":\n",
    "            #     close_pos = content_lower.find(\")\", temp_pos)\n",
    "            #     if close_pos != -1:\n",
    "            #         count += 1\n",
    "            #         start = close_pos + 1   #at the moment, this works for the scope of this project\n",
    "            # else:                           # Count even if no arguments\n",
    "            count += 1\n",
    "            start = end_pos\n",
    "\n",
    "        if count > 0:\n",
    "            feature_counts[feature] += count\n",
    "            feature_repo_map[feature].add(repo_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files saved: ehrQL_feature_counts.csv, feature-repo_map.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Save counts\n",
    "df_counts = pd.DataFrame(\n",
    "    [{\"Feature\": feat, \"Count\": feature_counts[feat]} for feat in features_to_search],\n",
    "    columns=[\"Feature\", \"Count\"]\n",
    ")\n",
    "df_counts.to_csv(\"ehrQL_feature_counts.csv\", index=False)\n",
    "\n",
    "repo_rows = []\n",
    "for feat, repos in feature_repo_map.items():\n",
    "    for repo in sorted(repos):\n",
    "        repo_rows.append({\"Feature\": feat, \"Repository\": repo, \"Raw_URL\": raw_url})\n",
    "\n",
    "df_repos = pd.DataFrame(repo_rows, columns=[\"Feature\", \"Repository, Raw_URL\"])\n",
    "df_repos.to_csv(\"feature-repo_map.csv\", index=False)\n",
    "\n",
    "logging.info(\"Feature counts and repo map exported.\")\n",
    "print(\"Files saved: ehrQL_feature_counts.csv, feature-repo_map.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
